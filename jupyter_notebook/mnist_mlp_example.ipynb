{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_mlp_example.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"9X_CD1Bcg7Ho","colab_type":"text"},"cell_type":"markdown","source":["# MNIST\n","\n","---\n","\n"]},{"metadata":{"id":"NC5s2CjYgqoX","colab_type":"toc"},"cell_type":"markdown","source":[">[Multi Layer Perceptron](#scrollTo=o-Oya0O8ZHMR)\n","\n",">[tf.keras](#scrollTo=TbTXeoq5ghjo)\n","\n"]},{"metadata":{"id":"o-Oya0O8ZHMR","colab_type":"text"},"cell_type":"markdown","source":["# Multi Layer Perceptron"]},{"metadata":{"id":"lZwvIlS8ZRi2","colab_type":"text"},"cell_type":"markdown","source":["Load tensorflow library and MNIST data"]},{"metadata":{"id":"C91hnTOtZWTG","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","# Import MNIST data\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n","\n","print('Test shape:',mnist.test.images.shape)\n","print('Train shape:',mnist.train.images.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5c4noXlkaOm7","colab_type":"text"},"cell_type":"markdown","source":["Neural network parameters"]},{"metadata":{"id":"C_W5AOA3aScD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","display_step = 1\n","\n","# Network Parameters\n","n_hidden_1 = 256 # 1st layer number of features\n","n_hidden_2 = 256 # 2nd layer number of features\n","n_input = 784 # MNIST data input (img shape: 28*28)\n","n_classes = 10 # MNIST total classes (0-9 digits)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3a3wgHrZaVxU","colab_type":"text"},"cell_type":"markdown","source":["Build graph"]},{"metadata":{"id":"hUxf0GSzaYn8","colab_type":"code","colab":{}},"cell_type":"code","source":["x = tf.placeholder(\"float\", [None, n_input])\n","y = tf.placeholder(\"float\", [None, n_classes])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yQ5m_aAeabbk","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create model\n","def multilayer_perceptron(x, weights, biases):\n","    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n","    print( 'x:', x.get_shape(), 'W1:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())        \n","    # Hidden layer with RELU activation\n","    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n","    layer_1 = tf.nn.relu(layer_1)\n","\n","    # Hidden layer with RELU activation\n","    print( 'layer_1:', layer_1.get_shape(), 'W2:', weights['h2'].get_shape(), 'b2:', biases['b2'].get_shape())        \n","    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n","    layer_2 = tf.nn.relu(layer_2)\n","\n","    # Output layer with linear activation\n","    print( 'layer_2:', layer_2.get_shape(), 'W3:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n","    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n","    print('out_layer:',out_layer.get_shape())\n","\n","    return out_layer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l4v97Jziajkc","colab_type":"text"},"cell_type":"markdown","source":["Initialize weights and construct the model"]},{"metadata":{"id":"xdoDW1YwakDM","colab_type":"code","colab":{}},"cell_type":"code","source":["# Store layers weight & bias\n","weights = {\n","    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),    #784x256\n","    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #256x256\n","    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))  #256x10\n","}\n","biases = {\n","    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #256x1\n","    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #256x1\n","    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n","}\n","\n","# Construct model\n","pred = multilayer_perceptron(x, weights, biases)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TP22RExGaqmL","colab_type":"text"},"cell_type":"markdown","source":["Define Loss function, and Optimizer"]},{"metadata":{"id":"OIIQkKznarFD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Cross entropy loss function\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n","\n","# On this case we choose the AdamOptimizer\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZJtBCmwiavwr","colab_type":"text"},"cell_type":"markdown","source":["Launch graph"]},{"metadata":{"id":"7aF6lnOBawLj","colab_type":"code","colab":{}},"cell_type":"code","source":["# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","# Launch the graph\n","with tf.Session() as sess:\n","    sess.run(init)\n","\n","    # Training cycle\n","    for epoch in range(training_epochs):\n","        avg_cost = 0.\n","        total_batch = int(mnist.train.num_examples/batch_size)\n","        # Loop over all batches\n","        for i in range(total_batch):\n","            batch_x, batch_y = mnist.train.next_batch(batch_size)\n","            # Run optimization op (backprop) and cost op (to get loss value)\n","            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n","                                                          y: batch_y})\n","            # Compute average loss\n","            avg_cost += c / total_batch\n","        # Display logs per epoch step\n","        if epoch % display_step == 0:\n","            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n","                \"{:.9f}\".format(avg_cost))\n","    print(\"Optimization Finished!\")\n","\n","    # Test model\n","    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n","    # Calculate accuracy\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n","    # To keep sizes compatible with model\n","    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TbTXeoq5ghjo","colab_type":"text"},"cell_type":"markdown","source":["# tf.keras"]},{"metadata":{"id":"5StwSxEKgEkY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"d487cbf0-8d51-495b-ae22-4759e7070895","executionInfo":{"status":"ok","timestamp":1534486194813,"user_tz":-420,"elapsed":49639,"user":{"displayName":"Anh Ng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"115197980876020762739"}}},"cell_type":"code","source":["import tensorflow as tf\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","model.evaluate(x_test, y_test)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Epoch 1/5\n","60000/60000 [==============================] - 10s 158us/step - loss: 0.2215 - acc: 0.9346\n","Epoch 2/5\n","60000/60000 [==============================] - 9s 154us/step - loss: 0.0961 - acc: 0.9703\n","Epoch 3/5\n","60000/60000 [==============================] - 9s 155us/step - loss: 0.0706 - acc: 0.9773\n","Epoch 4/5\n","60000/60000 [==============================] - 9s 155us/step - loss: 0.0525 - acc: 0.9836\n","Epoch 5/5\n","60000/60000 [==============================] - 9s 154us/step - loss: 0.0428 - acc: 0.9861\n","10000/10000 [==============================] - 1s 53us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.06444125682609156, 0.98]"]},"metadata":{"tags":[]},"execution_count":13}]}]}